import { ONNXNPUServer } from '../../servers/onnxNpuServer.js';
import { BaseAIService } from './baseAIService.js';

export class LocalLLMService extends BaseAIService {
  constructor() {
    super();
    this.endpoint = 'http://localhost:5834/api/chat/completions';
    this.onnxServer = null;
    this.isServerManaged = false;
  }

  async startServer() {
    try {
      // ã‚µãƒ¼ãƒãƒ¼ãŒæ—¢ã«èµ·å‹•ã—ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
      const healthCheck = await this.checkServerHealth();
      if (healthCheck.status === 'ok') {
        console.log('ğŸŸ¢ ONNX NPU Server is already running');
        return true;
      }

      console.log('ğŸ”µ Starting ONNX NPU Server...');
      this.onnxServer = new ONNXNPUServer();
      await this.onnxServer.start();
      this.isServerManaged = true;
      
      // ã‚µãƒ¼ãƒãƒ¼èµ·å‹•ç¢ºèª
      await this.waitForServer();
      console.log('âœ… ONNX NPU Server started successfully');
      return true;
      
    } catch (error) {
      console.error('âŒ Failed to start ONNX NPU Server:', error);
      return false;
    }
  }

  async stopServer() {
    if (this.onnxServer && this.isServerManaged) {
      console.log('ğŸ”´ Stopping ONNX NPU Server...');
      await this.onnxServer.stop();
      this.onnxServer = null;
      this.isServerManaged = false;
      console.log('âœ… ONNX NPU Server stopped');
    }
  }

  async checkServerHealth() {
    try {
      const response = await fetch(`http://localhost:5834/health`, {
        method: 'GET',
        timeout: 5000
      });
      return await response.json();
    } catch (error) {
      return { status: 'error', message: error.message };
    }
  }

  async waitForServer(maxRetries = 30, intervalMs = 1000) {
    for (let i = 0; i < maxRetries; i++) {
      const health = await this.checkServerHealth();
      if (health.status === 'ok' && health.model_loaded) {
        return true;
      }
      console.log(`â³ Waiting for server... (${i + 1}/${maxRetries})`);
      await new Promise(resolve => setTimeout(resolve, intervalMs));
    }
    throw new Error('Server failed to start within timeout period');
  }

  async callAI(prompt, debug = false) {
    const requestPayload = {
      messages: [
        {
          role: 'system',
          content: 'ã‚ãªãŸã¯æƒ…ã‚·ã‚¹éƒ¨é–€ãŒãƒ¦ãƒ¼ã‚¶ã‹ã‚‰å—ã‘ã‚‹ãƒ¡ãƒ¼ãƒ«ãŒã‚¯ãƒ¬ãƒ¼ãƒ ã‹ã©ã†ã‹ã‚’åˆ¤å®šã™ã‚‹å°‚é–€ã®AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚'
        },
        {
          role: 'user',
          content: prompt
        }
      ],
      max_tokens: 2000,
      temperature: 0.7,
      stream: false
    };

    if (debug) {
      console.log('\nğŸ¤– Local LLMã¸ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆ:');
      console.log('ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ:', this.endpoint);
      console.log('ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒšã‚¤ãƒ­ãƒ¼ãƒ‰:', JSON.stringify(requestPayload, null, 2));
      console.log('================================\n');
    }
    
    const response = await fetch(this.endpoint, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify(requestPayload)
    });

    if (!response.ok) {
      throw new Error(`HTTP error! status: ${response.status}`);
    }

    const data = await response.json();

    if (debug) {
      console.log('\nğŸ”§ ===== Local LLM DEBUG: ãƒ¬ã‚¹ãƒãƒ³ã‚¹ =====');
      console.log('ğŸ“Š ãƒ¬ã‚¹ãƒãƒ³ã‚¹çµ±è¨ˆ:');
      console.log('- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒˆãƒ¼ã‚¯ãƒ³æ•°:', data.usage?.prompt_tokens || 'N/A');
      console.log('- å®Œäº†ãƒˆãƒ¼ã‚¯ãƒ³æ•°:', data.usage?.completion_tokens || 'N/A');
      console.log('- ç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°:', data.usage?.total_tokens || 'N/A');
      console.log('- ãƒ¢ãƒ‡ãƒ«:', data.model || 'N/A');
      console.log('- ä½œæˆæ™‚é–“:', data.created || 'N/A');
      console.log('- ãƒ¬ã‚¹ãƒãƒ³ã‚¹ID:', data.id || 'N/A');
      console.log('- choicesé…åˆ—é•·:', data.choices?.length || 0);
      
      if (!data.choices || data.choices.length === 0) {
        console.log('âŒ è­¦å‘Š: choicesé…åˆ—ãŒç©ºã¾ãŸã¯undefined');
      } else {
        console.log('- é¸æŠã•ã‚ŒãŸchoiceã®index:', 0);
        console.log('- choice.finish_reason:', data.choices[0].finish_reason || 'N/A');
        console.log('- choice.message.role:', data.choices[0].message?.role || 'N/A');
        console.log('- choice.message.contenté•·:', data.choices[0].message?.content?.length || 0);
      }
      
      console.log('\nğŸ’¬ Local LLMã‹ã‚‰ã®ç”Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹:');
      const content = data.choices[0].message.content;
      if (!content) {
        console.log('âŒ è­¦å‘Š: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒnullã¾ãŸã¯undefined');
      } else if (content.trim() === '') {
        console.log('âŒ è­¦å‘Š: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒç©ºæ–‡å­—åˆ—');
      } else {
        console.log(content);
      }
      console.log('===================================\n');
    }

    return data.choices[0].message.content;
  }

  getServiceName() {
    return 'Local LLM';
  }
}